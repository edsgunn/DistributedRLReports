
@article{AlphaGo,
	abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses `value networks'to evaluate board positions and `policy networks'to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	date = {2016/01/01},
	date-added = {2022-11-26 12:09:29 +0000},
	date-modified = {2022-11-26 12:09:29 +0000},
	doi = {10.1038/nature16961},
	id = {Silver2016},
	isbn = {1476-4687},
	journal = {Nature},
	number = {7587},
	pages = {484--489},
	title = {Mastering the game of Go with deep neural networks and tree search},
	url = {https://doi.org/10.1038/nature16961},
	volume = {529},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1038/nature16961}}

@article{OpenAIFive,
  author    = {Christopher Berner and
               Greg Brockman and
               Brooke Chan and
               Vicki Cheung and
               Przemyslaw Debiak and
               Christy Dennison and
               David Farhi and
               Quirin Fischer and
               Shariq Hashme and
               Christopher Hesse and
               Rafal J{\'{o}}zefowicz and
               Scott Gray and
               Catherine Olsson and
               Jakub Pachocki and
               Michael Petrov and
               Henrique Pond{\'{e}} de Oliveira Pinto and
               Jonathan Raiman and
               Tim Salimans and
               Jeremy Schlatter and
               Jonas Schneider and
               Szymon Sidor and
               Ilya Sutskever and
               Jie Tang and
               Filip Wolski and
               Susan Zhang},
  title     = {Dota 2 with Large Scale Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1912.06680},
  year      = {2019},
  url       = {http://arxiv.org/abs/1912.06680},
  eprinttype = {arXiv},
  eprint    = {1912.06680},
  timestamp = {Wed, 03 Jun 2020 10:56:28 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1912-06680.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{GeneralisationInRL,
  author    = {Robert Kirk and
               Amy Zhang and
               Edward Grefenstette and
               Tim Rockt{\"{a}}schel},
  title     = {A Survey of Generalisation in Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/2111.09794},
  year      = {2021},
  url       = {https://arxiv.org/abs/2111.09794},
  eprinttype = {arXiv},
  eprint    = {2111.09794},
  timestamp = {Mon, 22 Nov 2021 16:44:06 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2111-09794.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{AlgorithmDistillation,
  doi = {10.48550/ARXIV.2210.14215},
  
  url = {https://arxiv.org/abs/2210.14215},
  
  author = {Laskin, Michael and Wang, Luyu and Oh, Junhyuk and Parisotto, Emilio and Spencer, Stephen and Steigerwald, Richie and Strouse, DJ and Hansen, Steven and Filos, Angelos and Brooks, Ethan and Gazeau, Maxime and Sahni, Himanshu and Singh, Satinder and Mnih, Volodymyr},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {In-context Reinforcement Learning with Algorithm Distillation},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}
@misc{MiniHack,
  doi = {10.48550/ARXIV.2109.13202},
  
  url = {https://arxiv.org/abs/2109.13202},
  
  author = {Samvelyan, Mikayel and Kirk, Robert and Kurin, Vitaly and Parker-Holder, Jack and Jiang, Minqi and Hambro, Eric and Petroni, Fabio and Küttler, Heinrich and Grefenstette, Edward and Rocktäschel, Tim},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {MiniHack the Planet: A Sandbox for Open-Ended Reinforcement Learning Research},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{XLand,
  author    = {Open Ended Learning Team and
               Adam Stooke and
               Anuj Mahajan and
               Catarina Barros and
               Charlie Deck and
               Jakob Bauer and
               Jakub Sygnowski and
               Maja Trebacz and
               Max Jaderberg and
               Micha{\"{e}}l Mathieu and
               Nat McAleese and
               Nathalie Bradley{-}Schmieg and
               Nathaniel Wong and
               Nicolas Porcel and
               Roberta Raileanu and
               Steph Hughes{-}Fitt and
               Valentin Dalibard and
               Wojciech Marian Czarnecki},
  title     = {Open-Ended Learning Leads to Generally Capable Agents},
  journal   = {CoRR},
  volume    = {abs/2107.12808},
  year      = {2021},
  url       = {https://arxiv.org/abs/2107.12808},
  eprinttype = {arXiv},
  eprint    = {2107.12808},
  timestamp = {Tue, 03 Aug 2021 09:18:07 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2107-12808.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{OpenEndedLearning,
  doi = {10.48550/ARXIV.2202.08266},
  
  url = {https://arxiv.org/abs/2202.08266},
  
  author = {Meier, Robert and Mujika, Asier},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Robotics (cs.RO), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Open-Ended Reinforcement Learning with Neural Reward Functions},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{CurriculumLearning,
  doi = {10.48550/ARXIV.2003.04960},
  
  url = {https://arxiv.org/abs/2003.04960},
  
  author = {Narvekar, Sanmit and Peng, Bei and Leonetti, Matteo and Sinapov, Jivko and Taylor, Matthew E. and Stone, Peter},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {Creative Commons Attribution 4.0 International}
}
@ARTICLE{NoFreeLunch,
  author={Wolpert, D.H. and Macready, W.G.},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={No free lunch theorems for optimization}, 
  year={1997},
  volume={1},
  number={1},
  pages={67-82},
  doi={10.1109/4235.585893}
  }


@misc{AdA,
  doi = {10.48550/ARXIV.2301.07608},
  
  url = {https://arxiv.org/abs/2301.07608},
  
  author = {{Adaptive Agent Team} and Bauer, Jakob and Baumli, Kate and Baveja, Satinder and Behbahani, Feryal and Bhoopchand, Avishkar and Bradley-Schmieg, Nathalie and Chang, Michael and Clay, Natalie and Collister, Adrian and Dasagi, Vibhavari and Gonzalez, Lucy and Gregor, Karol and Hughes, Edward and Kashem, Sheleem and Loks-Thompson, Maria and Openshaw, Hannah and Parker-Holder, Jack and Pathak, Shreya and Perez-Nieves, Nicolas and Rakicevic, Nemanja and Rocktäschel, Tim and Schroecker, Yannick and Sygnowski, Jakub and Tuyls, Karl and York, Sarah and Zacherl, Alexander and Zhang, Lei},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Human-Timescale Adaptation in an Open-Ended Task Space},
  
  publisher = {arXiv},
  
  year = {2023},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{ES,
  doi = {10.48550/ARXIV.1703.03864},
  
  url = {https://arxiv.org/abs/1703.03864},
  
  author = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya},
  
  keywords = {Machine Learning (stat.ML), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Evolution Strategies as a Scalable Alternative to Reinforcement Learning},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{EBCDQL,
  doi = {10.48550/ARXIV.2109.01417},
  
  url = {https://arxiv.org/abs/2109.01417},
  
  author = {Ornia, Daniel Jarne and Mazo, Manuel},
  
  keywords = {Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Multiagent Systems (cs.MA), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Event-Based Communication in Distributed Q-Learning},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{DAVIA,
  doi = {10.48550/ARXIV.2112.05908},
  
  url = {https://arxiv.org/abs/2112.05908},
  
  author = {Gatsis, Konstantinos},
  
  keywords = {Machine Learning (cs.LG), Systems and Control (eess.SY), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  
  title = {Federated Reinforcement Learning at the Edge},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{IWES,
      title={Importance Weighted Evolution Strategies}, 
      author={Víctor Campos and Xavier Giro-i-Nieto and Jordi Torres},
      year={2018},
      eprint={1811.04624},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{Atari,
author = {Bellemare, Marc G. and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
title = {The Arcade Learning Environment: An Evaluation Platform for General Agents},
year = {2013},
issue_date = {May 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {47},
number = {1},
issn = {1076-9757},
abstract = {In this article we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by developing and benchmarking domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. All of the software, including the benchmark agents, is publicly available.},
journal = {J. Artif. Int. Res.},
month = {may},
pages = {253–279},
numpages = {27}
}

@inproceedings{MuJoCo,
  title={MuJoCo: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE},
  doi={10.1109/IROS.2012.6386109}
}


@article{REINFORCE,
	abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
	author = {Williams, Ronald J. },
	date = {1992/05/01},
	date-added = {2023-04-16 21:14:48 +0100},
	date-modified = {2023-04-16 21:14:48 +0100},
	doi = {10.1007/BF00992696},
	id = {Williams1992},
	isbn = {1573-0565},
	journal = {Machine Learning},
	number = {3},
	pages = {229--256},
	title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
	url = {https://doi.org/10.1007/BF00992696},
	volume = {8},
	year = {1992},
	bdsk-url-1 = {https://doi.org/10.1007/BF00992696}}

@misc{TableTennis,
      title={i-Sim2Real: Reinforcement Learning of Robotic Policies in Tight Human-Robot Interaction Loops}, 
      author={Saminda Abeyruwan and Laura Graesser and David B. D'Ambrosio and Avi Singh and Anish Shankar and Alex Bewley and Deepali Jain and Krzysztof Choromanski and Pannag R. Sanketi},
      year={2022},
      eprint={2207.06572},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}


@article{AlphaStar,
	abstract = {Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1--3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8{\%} of officially ranked human players.},
	author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, R{\'e}mi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and W{\"u}nsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
	date = {2019/11/01},
	date-added = {2023-04-19 15:43:37 +0100},
	date-modified = {2023-04-19 15:43:37 +0100},
	doi = {10.1038/s41586-019-1724-z},
	id = {Vinyals2019},
	isbn = {1476-4687},
	journal = {Nature},
	number = {7782},
	pages = {350--354},
	title = {Grandmaster level in StarCraft II using multi-agent reinforcement learning},
	url = {https://doi.org/10.1038/s41586-019-1724-z},
	volume = {575},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1038/s41586-019-1724-z}}

@ARTICLE{TriplePendulum,
  author={Wu, Qingxiang and Sun, Ning and Yang, Tong and Fang, Yongchun},
  journal={IEEE Transactions on Industrial Electronics}, 
  title={Deep Reinforcement Learning-Based Control for Asynchronous Motor-Actuated Triple Pendulum Crane Systems With Distributed Mass Payloads}, 
  year={2023},
  volume={},
  number={},
  pages={1-10},
  doi={10.1109/TIE.2023.3262891}}

  @misc{RLHF,
      title={Training language models to follow instructions with human feedback}, 
      author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
      year={2022},
      eprint={2203.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{Vision,
      title={Tuning computer vision models with task rewards}, 
      author={André Susano Pinto and Alexander Kolesnikov and Yuge Shi and Lucas Beyer and Xiaohua Zhai},
      year={2023},
      eprint={2302.08242},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@ARTICLE{LAPG,
  author={Chen, Tianyi and Zhang, Kaiqing and Giannakis, Georgios B. and Başar, Tamer},
  journal={IEEE Transactions on Control of Network Systems}, 
  title={Communication-Efficient Policy Gradient Methods for Distributed Reinforcement Learning}, 
  year={2022},
  volume={9},
  number={2},
  pages={917-929},
  doi={10.1109/TCNS.2021.3078100}}

@misc{TDCompression,
      title={Temporal Difference Learning with Compressed Updates: Error-Feedback meets Reinforcement Learning}, 
      author={Aritra Mitra and George J. Pappas and Hamed Hassani},
      year={2023},
      eprint={2301.00944},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{NS-ES,
author = {Conti, Edoardo and Madhavan, Vashisht and Such, Felipe Petroski and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
title = {Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Evolution strategies (ES) are a family of black-box optimization algorithms able to train deep neural networks roughly as well as Q-learning and policy gradient methods on challenging deep reinforcement learning (RL) problems, but are much faster (e.g. hours vs. days) because they parallelize better. However, many RL problems require directed exploration because they have reward functions that are sparse or deceptive (i.e. contain local optima), and it is unknown how to encourage such exploration with ES. Here we show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents, specifically novelty search (NS) and quality diversity (QD) algorithms, can be hybridized with ES to improve its performance on sparse or deceptive deep RL tasks, while retaining scalability. Our experiments confirm that the resultant new algorithms, NS-ES and two QD algorithms, NSR-ES and NSRA-ES, avoid local optima encountered by ES to achieve higher performance on Atari and simulated robots learning to walk around a deceptive trap. This paper thus introduces a family of fast, scalable algorithms for reinforcement learning that are capable of directed exploration. It also adds this new family of exploration algorithms to the RL toolbox and raises the interesting possibility that analogous algorithms with multiple simultaneous paths of exploration might also combine well with existing RL algorithms outside ES.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {5032–5043},
numpages = {12},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@misc{Gym,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}

@misc{LocalMinima,
      title={Deep Learning without Poor Local Minima}, 
      author={Kenji Kawaguchi},
      year={2016},
      eprint={1605.07110},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{FederatedLearning,
title	= {Federated Learning: Strategies for Improving Communication Efficiency},
author	= {Jakub Konečný and H. Brendan McMahan and Felix X. Yu and Peter Richtarik and Ananda Theertha Suresh and Dave Bacon},
year	= {2016},
URL	= {https://arxiv.org/abs/1610.05492},
booktitle	= {NIPS Workshop on Private Multi-Party Machine Learning}
}


@InProceedings{MAML,
  title = 	 {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  author =       {Chelsea Finn and Pieter Abbeel and Sergey Levine},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1126--1135},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/finn17a/finn17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/finn17a.html},
  abstract = 	 {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.}
}

@article{NGD,
  title={Revisiting Normalized Gradient Descent: Fast Evasion of Saddle Points},
  author={Ryan W. Murray and Brian Swenson and Soummya Kar},
  journal={IEEE Transactions on Automatic Control},
  year={2017},
  volume={64},
  pages={4818-4824}
}

@MISC {UnfiormProof,
    TITLE = {Normalized vector of Gaussian variables is uniformly distributed on the sphere},
    AUTHOR = {Asaf Shachar (https://math.stackexchange.com/users/104576/asaf-shachar)},
    HOWPUBLISHED = {Mathematics Stack Exchange},
    NOTE = {URL:https://math.stackexchange.com/q/1864519 (version: 2020-05-18)},
    EPRINT = {https://math.stackexchange.com/q/1864519},
    URL = {https://math.stackexchange.com/q/1864519}
}


@article{Adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}


@article{QLearning,
	abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
	author = {Watkins, Christopher J. C. H. and Dayan, Peter},
	date = {1992/05/01},
	date-added = {2023-05-09 10:36:40 +0100},
	date-modified = {2023-05-09 10:36:40 +0100},
	doi = {10.1007/BF00992698},
	id = {Watkins1992},
	isbn = {1573-0565},
	journal = {Machine Learning},
	number = {3},
	pages = {279--292},
	title = {Q-learning},
	url = {https://doi.org/10.1007/BF00992698},
	volume = {8},
	year = {1992},
	bdsk-url-1 = {https://doi.org/10.1007/BF00992698}}

@ARTICLE{ETC,
  author={Tabuada, Paulo},
  journal={IEEE Transactions on Automatic Control}, 
  title={Event-Triggered Real-Time Scheduling of Stabilizing Control Tasks}, 
  year={2007},
  volume={52},
  number={9},
  pages={1680-1685},
  doi={10.1109/TAC.2007.904277}}
