\chapter{Introduction}

\section{Motivation}
We are surrounded by distributed systems that make up a significant proportion of the infrastructure we have come to rely on. 
From social media to electrical power networks, it is essential we understand and improve their inner workings to allow them to cope with the ever-increasing demands of the modern world. 
Distributed systems provide great advantages over centralized ones. 
One advantage is scalability, by distributing a workload we are able to handle more data, more users, and more complex tasks.
Distributed systems are also more resilient than centralized ones, they can continue to function when exposed to failures.
They also provide a boost to performance as they can process data faster and more efficiently through parallelization. 
However, as the size of these systems grow, they are, often bottlenecked by their communication bandwidth. To take full advantage of the scalability of distributed systems and reduce the cost associated with communication we must develop communication efficient algorithms that maximize the rate of learning while minimizing the number and size of the messages that are exchanged.

The field of reinforcement learning (RL), recently highlighted by some very high profile algorithms such as AlphaGo \cite{AlphaGo} and OpenAI Five \cite{OpenAIFive}, aims to find policies that maximize the reward achieved in an environment. The problem setting it provides is incredibly versatile, with applications including robotics \cite{TableTennis}, games \cite{AlphaStar}, control systems \cite{TriplePendulum}, natural language processing \cite{RLHF}, and computer vision \cite{Vision}. 

Inspired by this, distributed reinforcement learning (DRL) maintains the framework but distributes the learning process with the goal of speeding up training and allowing for the use of larger more complex models. However, as with all distributed systems, we may face a communication bottleneck leading to the desire to reduce how often agents communicate and how large the messages are when they do.
As well as this, in many scenarios in which the data itself is distributed across devices and sharing this data is prohibited, for example for privacy or legal reasons, we must find a way to train a model while adhering to these constraints. This is the problem addressed in federated reinforcement learning (FRL).

In distributed systems the bandwidth available for communication between nodes varies widely. It is therefore desirable to develop algorithms where we can trade communication for performance where necessary to suit the limitations imposed by the system.

\section{Current approaches}
\label{sec:CurrentApproaches}
To achieve good communication efficiency there exists a number of different avenues for exploration, highlighted by recent work in this field. These include, thresholded communication strategies, gradient compression and black-box optimization.
In thresholded communication we only communicate information which is sufficiently interesting or will provide a sufficient update to the system. Inspired by event triggered control (ETC) EBCDQL \cite{EBCDQL} is a Q-learning based method where agents only communicate samples where the difference between the estimated value of a state-action pair and the actual observed value, known as the temporal difference (TD) error, exceeds the threshold defined by a temporally discounted sum of past TD errors. This means only samples which provide a significant update to the Q value will be used resulting in good communication efficiency. 
However, due to the use of a tabular Q function this solution is not feasible for problems with large or continuous state or action spaces.
Similar to this DAVIA \cite{DAVIA} uses a linearly approximated value function where the approximate gradient of the objective function with respect to the weights is only communicated each episode if the approximate update to the objective function exceeds a threshold which increases over time. 
LAPG \cite{LAPG} builds on distributed policy gradient methods, reducing the amount of communication without reducing the quality of learning. It does this by only communicating approximate policy gradients if the squared norm of the increase in the difference between the last two samples of the policy gradient, known as innovation, from the last uploaded policy gradient exceeds a threshold known as the LAPG condition.

Gradient compression increases communication efficiency by reducing the size of the messages sent by the system. This is demonstrated in \cite{TDCompression}, the TD-EF algorithm is proposed in which the system communicates a compressed version of the TD error and uses error feedback from the true TD error to update future communications. Under technical assumptions the authors give guarantees of convergence to the optimal policy.

Black-box optimization techniques benefit from a great advantage in terms of communication efficiency as they only need to communicate a single scalar after each episode. This is because they consider the system as a pure input output process where the input is the policy parameters and the output is the discounted cumulative reward. Using this they optimize directly in policy space.
The Evolution Strategies (ES) \cite{ES} algorithm has show very strong results on a range of challenging reinforcement learning benchmarks such as Atari \cite{Atari} and MuJoCo \cite{MuJoCo}. It approximates a policy gradient by taking samples of the total reward achieved in trajectories with parameters perturbed around the current parameters. This is highly parallelizable due to the minimal communication bandwidth used and lack of computationally expensive calculations such as calculating gradients using backpropagation. It is also very versatile, being effective on both discrete and continuous problems, however, in environments where all agents often receive the same reward it can perform poorly with little exploration. 
To address this NS-ES \cite{NS-ES} optimizes the novelty of the behaviours the agent exhibits, thus the system learns behaviours that are not like any seen in past iterations. Combining this with the goal of maximizing reward NSR-ES weights the importance of optimizing reward and optimizing novelty leading to an agent which is less likely to get stuck in local optima. Finally, NSRA-ES uses an adaptive scheme to weight novelty and reward meaning the agent achieves very impressive rewards while also avoiding local minima.
ES exhibits poor data efficiency as it discards large batches of reward samples immediately after use. In response to this IW-ES \cite{IWES} reuses old reward samples through importance weighting. This leads to an acceleration of learning, however, with large learning rates the algorithm can become unstable.



\section{Objectives of the project}
The problem this project considers is that of a number of distributed agents acting in parallel instances of the same environment where they can communicate with a central learner to jointly solve a reinforcement learning problem. 
This architecture is shown in figure \ref{fig:Architecture}.
\begin{figure}
    \centering
    \includegraphics[width=0.6\columnwidth]{Figures/Architecture.png}
    \caption{The distributed reinforcement architecture used for this project. Agents interact with a single environment taking actions and receiving an observation of the next state and a reward back. Agents also interact with the central learner sending messages and receiving updated model parameters.}
    \label{fig:Architecture}
\end{figure}
% Specifically I consider environments in which the task is episodic, meaning the agent will eventually reach a terminal state, however, when this occurs is unknown.
There are two main objectives of this project, the first is to explore methods for communication efficient distributed reinforcement learning and compare them in simulation analysing their respective communication efficiency. The second is to develop a new algorithm for making distributed learning more efficient.
To evaluate how well a specific algorithm performs in section \ref{sec:Metrics} I define metrics based on the quality of learning and the amount of communication exhibited during training.


\section{Contributions}

I use the metrics defined in section \ref{sec:Metrics} to compare algorithms from section \ref{sec:CurrentApproaches} in a longitudinal study spanning multiple environments. 
I also perform a qualitative analysis of each of the algorithms highlighting their particular strengths and discussing what can be done to address their weaknesses.
As well as this analysis, in section \ref{sec:GeneralScheme}, I propose a new general adjustable probabilistic communications scheme for ES, one of the most promising algorithms, and discuss the reasoning behind it. I then demonstrate in section \ref{sec:GaussianScheme} how a number of assumptions can be made to make the scheme implementable.
Using this assumption, I show that if it were correct the expected number of agents communicating each episode is $m$ out of $n$.
Finally, I conduct experiments to compare this scheme with the original communication scheme and analyse the effect of varying its parameters. 
% As well as this I look into how well the assumptions made by the Gaussian scheme hold.

I show that the evolution strategies algorithm with probabilistic communication (ESPC) has strong performance compared to ES, often achieving higher rewards while only communicating a fraction of the amount. I theorize that this is due to the bias it exhibits towards higher rewards causing ESPC to explore promising areas of parameter space more quickly than ES.
As well as this I explore how varying ESPC's adjustable communication parameter affects its performance. I find that ESPC performs better with more communication and investigate unexpected communication behaviours at the beginning of training. Finally, I analyse how parameterizing the distribution I use to choose which samples are communicated affects the performance of ES. Specifically in the case of using a rolling mean and variance for this purpose I show that the horizon used for this has little effect on learning performance but causes a communication burn in period proportional to the length of the horizon.
