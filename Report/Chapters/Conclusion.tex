\chapter{Conclusion}

I have explored the current methods in distributed reinforcement learning and numerically compared them. Specifically I focused on evaluating, DQL, EBCDQL, DAVIA, and ES. I discussed how I implemented the algorithms by developing a framework to standardize their interfaces leading to a system where I could easily pair algorithms with environments in experiments. In the experiments I found that the value based methods are more suited to the Simple Grid environment than ES, finding effective solutions where ES failed to get any significant reward. However, on the Frozen Lake environment ES performed better where DQL and DAVIA were unable to find effective solutions. Furthermore, I discussed the drawbacks of the value based algorithms including the limitations of tabular methods and ways in which the algorithms could be amended to make them more suitable for complex environments. I discussed the communication performance of each of the algorithms highlighting the adaptive communication of EBCDQL and DAVIA as well as the small message sizes of ES. I concluded that despite its performance on Simple Grid, ES was the most versatile and promising of the algorithms, this was heavily influenced by past performance on famously challenging environments. In terms of communication efficiency the main drawback of ES is the frequency at which it sends messages.

To improve on this I proposed a probabilistic communication scheme in which samples with the highest magnitude were prioritized. I showed numerically that this scheme communicates less than the original communication scheme for evolution strategies while often performing better in terms of quality of learning. Specifically in the Cart Pole and Ant environments ESPC outperformed ES with the same number of agents while communicating half the number of messages.
I explored the effect of varying the communication parameter and the horizon length used to parameterize the utility distribution of ESPC. I found that increasing the amount of communication led to slower but more steady learning and decreasing it lead to fast erratic learning. In the case of very low amounts of communication I discussed how the approximation of the gradient can break down leading to a catastrophic collapse in learning performance. I also found that the horizon length had little effect on the learning performance of the algorithm but lead to closer tracking of reward by communication for longer horizons.

Future work could explore further choices of the utility function for ESPC and the assumed distribution induced by the function. In addition to this a mathematical analysis of the scheme could look to find how distributions of utility are induced by the choice of the utility function. Alternatively an approach could be to parameterize the utility function and use meta-learning for example MAML \cite{MAML} or a second layer of ES to optimize it. Another possibility would be to find a scheme in which the expected number of agents communicating at each episode $m$ varies adaptably during training, a way to achieve this could be to include the communication parameter in the model and update it as part of the ES update process. Finally, my work in this project has exclusively focused on the communication between agents and the central learner, however, with the efficiency of modern algorithms in this regard we are now limited by the communication of the new model parameters to from the central learner to the agents. Work exploring ways to compress these weights or only adaptably communicate them could lead to further increases in overall communication efficiency.