\chapter{Project}

\section{Problem setup}

We consider a CTDE DRL problem with $N$ agents acting in parallel instances of the same environment.
The agents are able to episodically communicate with a central learner in order to jointly solve a Reinforcement Learning problem.
They collect time series data consisting of state-action trajectories and rewards.

Consider the episodic MDP $\mathcal{M}= (\mathcal{S}^N, \mathcal{A}^N, \mathcal{P}, R,\gamma)$ where the terms are defined as in section \ref{sec:RL}.
A trajectory following policy $\pi$ through the environment is denoted as a sequence $\zeta = (s_0, a_1, r_1, s_1, a_2, \dots)$.
At the end of episode $k$ if some condition as a function of the current trajectory is satisfied i.e., $c^k_i=1$ where sample $c^k_i \leftarrow \mathcal{C}(\zeta^k_i), \ c^k_i \in \{0,1\}$ and $\mathcal{C}$ is some distribution representing a communication condition parameterized by the last trajectory (this could be deterministic), then the agent $i$ communicates information $z^k_i=Z(\zeta^1_i,\zeta^2_i,\dots, \zeta^k_i)$, derived from the trajectories it has experienced, to the central learner such as select $(s_{t-1},a_t,r_t,s_t)$ tuples or value gradients.
The central learner then updates the policy based on the information received and communicates this back to the agents. 
The event loop can be seen in algorithm \ref{alg:EventLoop}.


To evaluate the quality of DRL algorithms we establish metrics which can be used to compare them directly. These metrics broadly cover how well the systems learn and how much they communicate. In terms of communication we exclusively focus on messages sent from the agents to the central learner as in our problem the communication from central learner to agents is fixed. To evaluate how well an agent learns we can average the total reward achieved each episode across agents then plot the cumulative average total reward (CATR) againts episode number. The CATR as a function of episode number is 
\begin{equation*}
    \text{CATR}(k) = \sum_{l=1}^k \frac{1}{n} \sum_{i=1}^n \sum_{t} r^l_i(t)
\end{equation*}
where $r^l_i(t)$ is the reward in episode $l$ for agent $i$ at time $t$ or equivalent the reward in trajectory $\zeta^l_i$ at time $t$.
To analyse how an algorithm is performing on a particular episode we can look at the gradient of this line.
This is often clearer than plotting the average total reward, epecially on environmnets where rewards are very noisy.
We can say algorithm $a$ learns better than algorithm $b$ after $N$ training episodes if $\text{CATR}_a(N) > \text{CATR}_b(N)$.

As well as maximising reward we would like to minimize communication. For this we define two metrics. The first is the cumulative number of messages a system has sent on from agents to the central learner averaged over the number of agents (CAMN). The CAMN at episode $k$  is
\begin{equation*}
    \text{CANM}(k) = \sum_{l=1}^k \frac{1}{n} \sum_{i=1}^n c_i^l
\end{equation*} 
after $N$ training episodes. We say a algorithm $a$ communicates less frequently than algorithm $b$ if $\text{CAMN}_a(N) < \text{CAMN}_b(N)$. The second is cumualtive size in bytes of messages sent from agents to the central learner (CASM) after $N$ training episodes. The CASM at episode $k$ is 
\begin{equation*}
    \text{CASM}(k) = \sum_{l=1}^k \frac{1}{n} \sum_{i=1}^n \text{bytes}(z_i^l)
\end{equation*}  
We say algorithm $a$ communicates less intensely than algorithm $b$ if it $CASM_a(N) < CASM_b(N)$ after $N$ training episodes.


\begin{algorithm}
        \caption{Distributed RL Event Loop}\label{alg:EventLoop}
        \begin{algorithmic}
                \State Initialize $N$ \Comment{The number of episodes to be run}
                \State Initialize $\pi$
                \For {$k=1,\dots,N$} 
                \State central learner communicates $\pi$ to all agents
                \For {each agent $i = 1,\dots n$}
                \State run episode to collect sample trajectory $\zeta^k_i$ %\Comment{agents may adjust their policy during the episode}
                \State compute $z^k_i = Z(\zeta^1_i,\zeta^2_i,\dots, \zeta^k_i)$
                \State communicate $z^k_i$ if $c^k_i = 1$
                \EndFor
                \State update $\pi$ from received information
                \EndFor
        \end{algorithmic}
\end{algorithm}

\section{New algorithm}
\subsection{Process for calculating the gradient}
When calculating the gradient using evolutionary strategies we first take $n$ independent samples from a normal distribution by which we perturb the parameters $\theta$ of each agent $\epsilon_i \leftarrow \mathcal{N}(0,I), \ \forall i=1,\dots,n$. Then using these samples each agent runs an episode, sampling the cumulative discounted reward for each perturbation, $F_i \leftarrow \mathcal{F}(\theta+\sigma \epsilon_i), \ \forall i=1,\dots,n$ where $\mathcal{F}$ is some unknown distribution defined by the environment. Note that since the samples $(\epsilon_1,\dots\epsilon_n)$ and independent and identically distributed (iid) and sample $F_i$ is conditional on $\epsilon_i$ then the samples $(F_1,\dots,F_n)$ are also (iid). These samples are then communicated to the central learner and the approximate policy gradient is calculated as in equation \ref{eq:grad}.

We are intested in finding a scheme that reduces the number of samples that are communicated after each episode. Specifically we would only like $m$ out of $n$ agents to communicate or equivailantly $m-n$ out of $n$ agents to not communicate each episode on average. To do this we introduce the notion of the utility of a sample in calculating the final gradient, meaning how important is this particular sample relative to the others. Difficulty in determining this arises as a particular agent $i$ only knows the values of its own sample pair $(\epsilon_i, F_i)$. However, since the samples are iid, if we assume some distribution over the utility of the samples, it is possible to estimate the probability that the sample we have drawn is greater than that of at least $n-m$ out of $n-1$ other samples i.e., it is more useful in calculating the gradient than at least $n-m$ out of $n-1$ other samples. We can then communicate the sample with this probability.

\subsection{General Communication Scheme}
\label{sec:GeneralScheme}
To construct a communication scheme of this nature we must first specify some expected number of agents $m$ that we wish to communicate each episode.
We then define a deterministic utility function $U: S_\epsilon \times S_F \rightarrow \mathbb{R}$ where $S_\epsilon$ and $S_F$ are the sample spaces of the distributions from which the samples $\epsilon_i$ and $F_i$ are respectively drawn.
Since $U$ is a function of random variables, it is itself a random variable with its own marginal distribution. 
However, since the distribution $\mathcal{F}$ is unknown then the distribution $\mathcal{U}$ of $U$ is also unknown. 
We therefore assume some distribution of $U$ based on information the agent knows. 
This includes all historical values for utility the agent has experienced.
The procedure for determining whether a sample is communicated is then as follows
\begin{enumerate}
    \item Draw sample $\epsilon \leftarrow \mathcal{N}(0,I)$
    \item Draw sample $F \leftarrow \mathcal{F}(\theta + \sigma \epsilon)$
    \item Calculate $u = U(\epsilon,F)$
    \item Use assumed distribution $\mu \sim \mathcal{U}$ to calculate $p=\mathbb{P}(\mu < u)$
    \item Calculate $p_{comm}=\mathbb{P}(M>n-m) = \sum_{n-m+1}^{n-1} \begin{pmatrix}n-1 \\ k\end{pmatrix}p^k(1-p)^{n-k-1}$, where $M \sim \text{Bi}(n-1,p)$
    \item Generate a random number $\delta \leftarrow \text{Uniform}(0,1)$
    \item If $\delta<p_{comm}$ communicate $F$ to the central learner
\end{enumerate}

\subsection{Determining the utility function}
By discarding samples we will inevitably introduce some form of bias in the approximate policy gradient that we calculate. 
However, by careful choice of the utility function we can attempt to minimise the impact of this bias on the gradient ascent process.
When performing gradient ascent introducing bias in the direction will be more impactful than if we introduce bias to the magnitude. 
It is much worse to go in completely the wrong direction than it is to step too far or not far enough in the right one. 
This is highlighted by the popularity of normalized gradient decent/ascent \cite{NGD} in which the magnitude is discarded, entirely replaced by the manually assigned learning rate.
To calculate the gradient we use a sum of perturbation vectors weighted by the rewards they recieve. We can alternatively think of this as a weighted sum of direction vectors.

\begin{equation*}
    \sum^n_{i=1} F_i \|\epsilon_i\|_2 \hat{\epsilon}_i = \sum^n_{i=1} w_i \hat{\epsilon}_i.
\end{equation*}
Since the directions of $\epsilon_i$ are unifromly distributed on a hypersphere \cite{UnfiormProof} and each agent does not know what the others are, this direction $\hat{\epsilon_i}$ is not useful for an agent in determining how much a vector will contribute to the sum. We are thefore left with only useful factor being the magnitude, naturally leading to the utility function
\begin{equation*}
    U(F,\epsilon) = F \|\epsilon\|_2,
\end{equation*}
which can be easily calculated by the agent.

% \subsection{Utility as an optimisation problem}
% Maximise the expected value of c?
% \subsection{To be discarded if nothing can be proven}
% If we imagine an additional sample $c_i \in \{0,1\}$ for each sample pair $(\epsilon_i, F_i)$ where $\mathbb{P}(c=1)=p$ we can represent the vector part of gradient that we calculate as $\sum_{i=1}^n c_i F_i \epsilon_i$. We would like to minimise the direction between this vector and the vector we would have had if we were to communicate all samples $\sum_{i=1}^n F_i \epsilon_i$. This is equivalent to saying that we would like to maximise the expected normalised dot product of those two sums with respect to the function $U$ as the  normalised dot product is proportional to the cosine of the angle between the vectors.


% We can frame this as an unconstrained functional optimization problem over $U(J,\epsilon)$

% \begin{align*}
%     \max_U \  &\mathbb{E} \begin{bmatrix}\frac{(\sum_{i=1}^n c_i F_i \epsilon_i)^\top(\sum_{j=1}^n F_j \epsilon_j)}{ \|\sum_{i=1}^n c_i F_i \epsilon_i\|_2 \|\sum_{j=1}^n F_j \epsilon_j\|_2 } \end{bmatrix}\\
%     \text{with} &  \\
%     &\epsilon_i \sim \mathcal{N}(0,I), \forall i=1,\dots,m \\
%     & F_i \sim \mathcal{F}(\theta + \sigma \epsilon_i) \\
%     & u_i = U(\epsilon_i, F_i) \Rightarrow u_i \sim \mathcal{U}(\epsilon_i ,F_i)\\
%     & s_i = \mathbb{P}(\mathbb{E}[uj < u_i | u_i) \\
%     & t_i \sim \text{Bi}(n,s_i) \\
%     & c_i \sim \text{B}(\mathbb{P}(t_i \geq n-m))
% \end{align*}
% where the objective function is the expected cosine of the angle between the approximate policy gradient with and without the probibalistic communication scheme. It can be transformed to

% \begin{equation*}
%     \sum_{i=1}^n \sum_{j=1}^n \sum_{k=1}^n \sum_{l=n-m}^n \mathbb{E} [ \begin{pmatrix}m \\ k\end{pmatrix} s(U(\epsilon,F))^k(1-s(U(\epsilon,F)))^{m-k} F_i F_j \epsilon_{i,k} \epsilon_{j,k}] 
% \end{equation*}

\subsection{Naive Scheme}
\label{sec:NaiveScheme}

To be able to implement the general communication scheme we must assume some distribution of utility. This will allow us to calculate the probability that an angent's sample has higher utility than another and thus how likely it is to be communicated. The distribution of utility $\mathcal{U}$ is dependent on both the distribution of perturbation $\mathcal{N}(0,I)$ and reward $\mathcal{F}(\theta +\sigma \epsilon)$. The distribution of reward is unknown but we know is that it is conditional on the distribution of pertrubation which is gaussian. This gives us motivation to naively assume that utility is also distributied normally. To parameterize this normal distribution we can use a rolling mean and varaince of the last $k$ utility samples taken by an agent. Thus we assume $\mathcal{U} = \mathcal{N}(\bar{u}_k, u^{\sigma^2}_k)$ where $\bar{u}_k = \sum_{t=T-k}^T u^t$, $u^{\sigma^2}_k = \sum_{t=T-k}^T (u^t - \bar{u}_k)^2$ and $T$ is the current time step. Alternatively to this we could take a weighted averages of utilities weighting recent onces more highly.


\section{Experiments}
% We conducted experiements to compare a number of algorithms on a number of different environments as well as directly comparing ES with the original communication scheme with the proposed probabilistic scheme. In the direct comparisons we used the Frozen Lake environment from Gymnasium (formerly OpenAI Gym \cite{Gym}) environment CartPole and a third party environment Flappy Bird. To run these experiements easily we developed a framework so that we could rapidly swap out algorithms, evironments and the number of agents being tested. All experiments were run five times and the results presented are a mean over all 5 runs. 

\subsection{Algorithm comparison}
To gain an understanding performance of ESPC we compared it to the DQL, EBCDQL, DAVIA and ES algorithms in the Frozen Lake environment from Gymnasium (formerly OpenAI's Gym \cite{Gym}) as well as the custom environment Windy Grid. Frozen lake (figure \ref{fig:FrozenLake}) is a 5x5 Grid environmnet in which the goal is to move from the upper left to the lower right square upon which the agent recives a reward of $1$. A number of squares in the grid are holes in the lake, where if reached by the agent the episode ends. 

\begin{figure}
    \centering
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/frozen_lake.png}
        \caption{The frozen lake environment}
        \label{fig:FrozenLake}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/cart_pole.png}
        \caption{The cart pole environment}
        \label{fig:CartPole}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/flappy_bird.png}
        \caption{The flappy bird environment}
        \label{fig:FlappyBird}
    \end{subfigure}
\end{figure}

Windy Grid is a 5x5 gridworld where the objective is for the agent to move from the upper left square to the lower right. The agent recieves $-1$ reward every step except the final step in which it recieves a reward of $0$. The agent can move left, right, up, down or stay in the same place. At each step there is a $30\%$ chance of being pushed one square in the directions up and/or right in addition to the movement from the action taken. For the ES and ESPC policy we used a feedforward neural network with 2 hidden layers of size 10 as well as a momentum optimizer.
We carried out training over 1000 episodes and repeated the trial 5 times averaging our results over the trials. 

\subsection{ESPC evaluation}
EPSC is designed to reduce the communication between the agents and the central learner to an amount equivalent to that of ES with $m$ agents. 
We are therefore interested in evaluating this against an instance of ESPC which has $n$ agents but communicates like it has $m$. 
For ESPC to be effective it must learn better than ES with $m$ agents, otherwise it would show it is wasting computational resources for no benefit. 
In our experiment we also include an instance of ES with $n$ agents as a benchmark for performance. 
We therefore hypothesise that the performance of ESPC to lie somewhere between that of ES with $n$ and $m$ agents.

To test this hypothesis we conducted an experiment of this nature on the Gymnasium Cart Pole (figure \ref{fig:CartPole}), the MuJoCo Ant environment and third party Flappy Bird (figure \ref{fig:FlappyBird}) envrionment using $n=50$ and $m=25$. We used the naive assumption of utility distribution stated in section \ref{sec:NaiveScheme} and set the rolling average parameter $k=5$. The policy for both ES and ESPC was a feedforward nerual network with 2 hidden layers of size $8,64,$. We used Adam optimizer \cite{Adam} with a learning rate of $\alpha=0.01,0.005,$, an $L_2$ regularization coefficient of $0.005,0.005,0.005$, a pertubation standard deviation of $\sigma=0.05,0.02,$ and a discount factos of $0.999,0.99,$ where the parameters are for the Cart Pole, Ant and Flappy Bird environments respectively. Training was carried out over 200 episodes and repeated in 3 trials averaging over the results from the trials.
\label{sec:ESPCEval}

\subsection{Varying communication in ESPC}
A key advantage of ESPC over ES is the ability to tune the expected amount of communication on each episode. However, reducing communication introduces bias to the esimate of the gradient and will thus effect the quality of learning achieved. We therefore conducted an experiement where we varied the parameter $m$ hypothesising that quality of learning will decrease with the amount of communication.

We used $n=30$, $m=5,10,15,20$ and the naive assumption stated in section \ref{sec:NaiveScheme} with $k=10$. We included an instance of ES with $n=30$ as a benchmark for learning performance. The other details of the systems were the same as in section \ref{sec:ESPCEval}.

\subsection{Effect of naive utility distribution parameters}

To evaluate how the length of the rolling average has on the quality of the naive assumption of a normal utility distribution from section \ref{sec:NaiveScheme} we conducted an experiment where we varied $k$ with all other parameters fixed. We used $k=3,10,30$ with $n=30$, $m=15$ and compared against an instance of ES with $n=30$. All other system details were the same as in section \ref{sec:ESPCEval}. We trained over 1000 episodes and averaged our results across 5 trials.

% \subsection{Effect of utility distribution}

% We compare a number of distributions for utility WHICH ONES? against the naive distribution from \ref{sec:NaiveScheme} to evalutate which is preferable in which environments. The distributions we use are LIST DISTRIBUTIONS
    
\subsection{Computational limitations}
Due to having very limited amounts of computation at our disposal we were not able to conduct experiments of the scale we desired. Ideally for an algorithm like ES we would use a number of agents at least an order of magnitude higher and train on an order of magnitude more episodes, meaning we would be able to compare with ESPC on the complex environment such as those from the  MuJoCo \cite{MuJoCo} and Atari \cite{Atari} suits that ES proved particulaly effective on. Comparison would also have benefitted from the use of larger models such as CNNs for use on the Atari suit. However, we still feel the experiements that were conducted give an accurate picture of the performance of EPSC compared to ES and hope for future work to test in more complex environments.

\section{Results and Discussion}

\subsection{Algorithm comparison}

Relative to the value based algorithms (DQL, EBCDQL, DAVIA) on these environments ES and ESPC performed poorly in terms of quality of learning. Figure \ref{fig:AlgsWG} shows that they both fail to learn an effective strategy taking over 100 steps on average to reach the goal in windy grid, ten times more than DAVIA took on average. They performed similarly in this respect on the frozen lake environment shown in figure \ref{fig:AlgsFL} although in this case only EBCDQL performed well. The communication performance for ESPC was very good, only communicating more than DAVIA on both the number of messages sent and the cumulative size of those messages. 

ES has showed strong performance in the past on much more complex environments than those tested here. The reason that ES and ESPC fail to learn in the experiments presented is due to their lack of exploration. In order to get any non-negative reward at all in these environments an agent must randomly come across the terminal state. This is because all of the rewards recieved by agents for ES and ESPC will be the same resulting in no expected update to the parameters and in the case of frozen lake the reward will be 0 unless the terminal state is reached meaning the parameters will not update at all until a solution to the problem is found randomly. A potential remedy to this on these envrionments is to use novelty to encourage exploration as suggested in section \cite{NS-ES} and discussed in \ref{sec:CurrentApproaches}.

\begin{figure}
    \centering
    \begin{subfigure}{0.6\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/AllAlg/30/WindyGrid/reward.png}
        \caption{The cumulative average reward recieved by agents}
        \label{fig:AlgsWGReward}
    \end{subfigure}
    \begin{subfigure}{0.6\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/AllAlg/30/WindyGrid/num_messages.png}
        \caption{The cumulative number of messages sent from agents to central learner}
        \label{fig:AlgsWGNumMessages}
    \end{subfigure}
    \begin{subfigure}{0.6\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/AllAlg/30/WindyGrid/size_messages.png}
        \caption{The cumulative size of messages sent from agents to central learner}
        \label{fig:AlgsWGSizeMessages}
    \end{subfigure}
    \caption{Results from the algorithm comparison in the windy grid environment for DAVIA, DQL, EBCDQL, ES, and ESPC with $n=30$ over 1000 episodes}
    \label{fig:AlgsWG}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}{0.6\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/AllAlg/30/FrozenLake/reward.png}
        \caption{The cumulative average reward recieved by agents}
        \label{fig:AlgsFLReward}
    \end{subfigure}
    \begin{subfigure}{0.6\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/AllAlg/30/FrozenLake/num_messages.png}
        \caption{The cumulative number of messages sent from agents to central learner}
        \label{fig:AlgsFLNumMessages}
    \end{subfigure}
    \begin{subfigure}{0.6\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/AllAlg/30/FrozenLake/size_messages.png}
        \caption{The cumulative size of messages sent from agents to central learner}
        \label{fig:AlgsFLSizeMessages}
    \end{subfigure}
    \caption{Results from the algorithm comparison in the frozen lake environment for DAVIA, DQL, EBCDQL, ES, and ESPC with $n=30$ over 1000 episodes}
    \label{fig:AlgsFL}
\end{figure}

\subsection{ESPC evaluation}
\label{sec:ESPCDirect}
EPSC performed well in terms of both quality of learning and communication when compared to ES. Results on the cart pole environment are shown in figure \ref{fig:DirectCP}. ESPC with $n=50$ achieves a CATR less than that of ES with both $n=50$ and $n=25$ and a CANM similar to ES with $n=25$. It can been seen in figure \ref{fig:CPEpisodicReward} that ESPC converges to optimal reward slower than ES with higher variance, although ES is at near optimal reward for the whole training run indicating that this environment is quite simple to solve so a more complex one may give a better picture. The results on the Ant environment are shown in figure \ref{fig:DirectAnt} where ESPC with $n=50$ achieved a CATR between that of ES with $n=50$ and $n=25$ and a CANM in line with that of ES with $n=25$. Figure \ref{fig:AntEpisodicReward} shows that by the end of training ESPC is recieving a reward greater than that of both ES instances and has a general upward trend. The results on the flappy bird environment, figure \ref{fig:DirectFB}, show similar behaviour, however, in this case ES achieves a slightly lower cumulative reward, and by the end of training has a higher cumulaitve reward per episode than ES.
\begin{figure}
    \centering
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/ESPC/Direct_vec/CartPole/reward.png}
        \caption{The cumulative average reward recieved by agents}
        \label{fig:CPReward}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/ESPC/Direct_vec/CartPole/episodic_reward.png}
        \caption{The episodic average reward recieved by agents}
        \label{fig:CPEpisodicReward}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/ESPC/Direct_vec/CartPole/num_messages.png}
        \caption{The cumulative number of messages sent from agents to the central learner}
        \label{fig:CPMessages}
    \end{subfigure}
    \caption{Results for the comparison of ES with $n=25,50$ and ESPC with $n=50$, $m=25$ and $k=5$ on the cart pole environment over 200 episodes}
    \label{fig:DirectCP}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/ESPC/Direct_vec/Ant/reward.png}
        \caption{The cumulative average reward recieved by agents}
        \label{fig:AntReward}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/ESPC/Direct_vec/Ant/episodic_reward.png}
        \caption{The episodic average reward recieved by agents}
        \label{fig:AntEpisodicReward}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/ESPC/Direct_vec/Ant/num_messages.png}
        \caption{The cumulative number of messages sent from agents to the central learner}
        \label{fig:AntMessages}
    \end{subfigure}
    \caption{Results for the comparison of ES with $n=25,50$ and ESPC with $n=50$, $m=25$ and $k=5$ on the ant environment over 200 episodes}
    \label{fig:DirectAnt}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/ESPC/Direct/FlappyBird/reward.png}
        \caption{The cumulative average reward recieved by agents}
        \label{fig:FBReward}
    \end{subfigure}
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/ESPC/Direct/FlappyBird/num_messages.png}
        \caption{The cumulative number of messages sent from agents to the central learner}
        \label{fig:FBMessages}
    \end{subfigure}
    \caption{Results for the comparison of ES and ESPC on the flappy bird environment with $n=30$ where the communication parameter is varied $m=5,10,15,20,25$ over 1000 episodes}
    \label{fig:DirectFB}
\end{figure}

\subsection{Varying communication in ESPC}

The results achieved by varying the communication parameter $m$ on the cart pole environment are presented in figure \ref{fig:CPComm}. It can be seen that for all values of $m$ exept $m=5$ EPSC outperforms ES in terms of CATR with CAMN roughly equal to $m/n$ times that of ES per episode. However much like in section \ref{sec:ESPCDirect} many of the algorithms have near optimal reward for the whole of training indicating testing on a more complex environment would provide clearer results.
\begin{figure}
    \centering
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/ESPC/Comm_vec/CartPole/reward.png}
        \caption{The cumulative average reward recieved by agents}
        \label{fig:CommReward}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/ESPC/Comm_vec/CartPole/episodic_reward.png}
        \caption{The episodic average reward recieved by agents}
        \label{fig:CommEpisodicReward}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/ESPC/Comm_vec/CartPole/num_messages.png}
        \caption{The cumulative number of messages sent from agents to the central learner}
        \label{fig:CommMessages}
    \end{subfigure}
    \caption{Results for the comparison of ES and ESPC on the cart pole environment with $n=50$ where the communication parameter is varied $m=5,10,15,30,45$ over 200 episodes}
    \label{fig:CPComm}
\end{figure}


This is further shown in the results from the flappy bird environmnet in figure \ref{fig:CommFlappyReward}. Both ES and ESPC performed poorly, with ESPC always achieveing a lower cumulative reward over episodes, however, ES achieved the same average reward of $8.3$ on every episode, showing no exploration at all, whereas the variance of rewards achieved by ESPC was high sometimes achieveving over $16$ but often achieving $0$.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/ESPC/Comm/Flappy/reward.png}
    \caption{The cumulative reward recieved by ES and ESPC on the flappy bird environmnet over 1000 episodes with $n=30$ where $m=5,10,15,20,25$}
    \label{fig:CommFlappyReward}
\end{figure}

\subsection{Effect of naive utility distribution horizon}

Figure \ref{fig:CPHorizon} shows the results of varying the horizon of the samples with which we estimate the utility distribution. All instances recieve near optimal reward for the whole of training except ESPC with $k=5$ which only reaches this point around episode 180.
% In varying the horizon with which we calculate the parameters for the utility distribution for ESPC the performance increases as the horizon is decreased. This is shown in figure \ref{fig:Horizon}. We theroize this is because relying on recent utility samples is more likely to give an accurate representation of the utility distribution. This has led us to believe that using a weighted mean and standard deviation as parameters for the utility distibution where recent samples are weighted more highly would also be effective.

\begin{figure}
    \centering
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/ESPC/Horizon_vec/CartPole/reward.png}
        \caption{The cumulative average reward recieved by agents}
        \label{fig:HorizonReward}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/ESPC/Horizon_vec/CartPole/episodic_reward.png}
        \caption{The episodic average reward recieved by agents}
        \label{fig:HorizonEpisodicReward}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/ESPC/Horizon_vec/CartPole/num_messages.png}
        \caption{The cumulative number of messages sent from agents to the central learner}
        \label{fig:HorizonMessages}
    \end{subfigure}
    \caption{Results for the comparison of ES and ESPC on the cart pole environment with $n=30$ and $m=15$ where the rolling average horizon parameter is varied $k=5,10,30$ over 200 episodes}
    \label{fig:CPHorizon}
\end{figure}

% \subsection{Qualitative comparison}

\section{Conclusion}

We have explored the current methods in distributed reinforcement learning and both qualitiatively and numerically compared them. From our experiments we found that FINDINGS. Furthermore we discussed in our qualitative comparison that WHAT DID WE DISCUSS. From this we concluded that evolution strategies was the most promising algorithm for this application, however, its communication performance in terms of number of messages sent by agents was poor. To improve on this we proposed a probabilistic communication scheme in which samples with the highest magnitude were prioritised. We showed numerically that this scheme communicates less than the original communication scheme for evolution strategies while often performing better in terms of quality of learning.

Future work could explore further choices of the utility function for ESPC and the assumed disribution induced by the function. In addition to this a mathematical analysis of the scheme could look to find how distributions of utility are induced by the choice of function. Alternatively an approach could be to parameterize the utility function and use meta-learning for example MAML \cite{MAML} to optimize it. Another possibility would be to find a scheme in which the expected number of agents communcating at each episode $m$ varies adaptably during training. Finally our work in this project has exclusively focused on the communication between agents and the central learner, however, with the efficiency of modern algorithms in this regard we are now limited by the communication of the new model parameters to from the central learner to the agents. Work exploring ways to compress these weights or only adaptably communicate them could lead to further increases in overall communication efficiency.