\chapter{Comparative Analysis}

\section{Problem setup}

I consider a CTDE DRL problem with $N$ agents acting in parallel instances of the same environment.
The agents are able to episodically communicate with a central learner in order to jointly solve a Reinforcement Learning problem.
They collect time series data consisting of state-action trajectories and rewards.

Consider the episodic MDP $\mathcal{M}= (\mathcal{S}^N, \mathcal{A}^N, \mathcal{P}, R,\gamma)$ where the terms are defined as in section \ref{sec:RL}.
A trajectory following policy $\pi$ through the environment is denoted as a sequence $\zeta = (s_0, a_1, r_1, s_1, a_2, \dots)$.
At the end of episode $k$ in trial $l$ if some condition as a function of the current trajectory is satisfied i.e., $c^{k,l}_i=1$ where sample $c^{k,l}_i \leftarrow \mathcal{C}(\zeta^{k,l}_i), \ c^{k,l}_i \in \{0,1\}$ and $\mathcal{C}$ is some distribution representing a communication condition parameterized by the latest trajectory (this could be deterministic), then the agent $i$ communicates information $z^{k,l}_i=Z(\zeta^{1,l}_i,\zeta^{2,l}_i,\dots, \zeta^{k,l}_i)$, derived from the trajectories it has experienced, to the central learner such as select $(s_{t-1},a_t,r_t,s_t)$ tuples or value gradients.
The central learner then updates the policy based on the information received and communicates this back to the agents. 
The event loop can be seen in algorithm \ref{alg:EventLoop}.

\begin{algorithm}
    \caption{Distributed RL Event Loop}\label{alg:EventLoop}
    \begin{algorithmic}
            \State Initialize $L$ \Comment{The number of trials to be run}
            \State Initialize $N$ \Comment{The number of episodes to be run}
            \For {$l=1,\dots,L$}
            \State Initialize $\pi$
            \For {$k=1,\dots,N$} 
            \State central learner communicates $\pi$ to all agents
            \For {each agent $i = 1,\dots n$}
            \State run episode to collect sample trajectory $\zeta^{k,l}_i$ %\Comment{agents may adjust their policy during the episode}
            \State compute $z^{k,l}_i = Z(\zeta^{1,l}_i,\zeta^{2,l}_i,\dots, \zeta^{k,l}_i)$
            \State communicate $z^{k,l}_i$ if $c^{k,l}_i = 1$
            \EndFor
            \State update $\pi$ from received information
            \EndFor
            \EndFor
    \end{algorithmic}
\end{algorithm}

\section{Metrics}
\label{sec:Metrics}
To evaluate the quality of DRL algorithms I establish metrics which can be used to compare them directly. These metrics broadly cover how well the systems learn and how much they communicate. In terms of communication I exclusively focus on messages sent from the agents to the central learner as in this problem the communication from central learner to agents is fixed. To evaluate how well an agent learns we can average the discounted episodic reward achieved each episode across agents and trials then plot the average discounted episodic reward ($\mu^{\text{reward}}$) against episode number. The $\mu^{\text{reward}}$ as a function of episode number is 
\begin{equation*}
    \mu^{\text{reward}}(k) = \frac{1}{L} \sum_{l=1}^L \frac{1}{n} \sum_{i=1}^n \sum_{t} \gamma^t r^{k,l}_i(t)
\end{equation*}
where $r^{k,l}_i(t)$ is the reward in episode $k$ and trial $l$ for agent $i$ at time $t$ or equivalent the reward in trajectory $\zeta^{k,l}_i$ at time $t$.
We can say algorithm $a$ learns better than algorithm $b$ after $N$ training episodes if $\mu^{\text{reward}}_a(N) > \mu^{\text{reward}}_b(N)$.

As well as maximizing reward we would like to minimize communication. For this we define two metrics. The first is the number of messages a system has sent from agents to the central learner averaged over the number of agents ($\mu^{\text{frequency}}$). The $\mu^{\text{frequency}}$ at episode $k$  is
\begin{equation*}
    \mu^{\text{frequency}}(k) = \frac{1}{L} \sum_{l=1}^L \frac{1}{n} \sum_{i=1}^n c_i^{k,l}
\end{equation*} 
We say an algorithm $a$ communicates less frequently than algorithm $b$ on episode $N$ if $\mu^{\text{frequency}}_a(N) < \mu^{\text{frequency}}_b(N)$. The second is size in bytes of messages sent from agents to the central learner averaged over the number of agents ($\mu^{\text{size}}$). The $\mu^{\text{size}}$ at episode $k$ is 
\begin{equation*}
    \mu^{\text{size}}(k) = \frac{1}{L} \sum_{l=1}^L \frac{1}{n} \sum_{i=1}^n \text{bytes}(z_i^{k,l})
\end{equation*}  
We say algorithm $a$ communicates less intensely than algorithm $b$ on episode $N$ if $\mu^{\text{size}}_a(N) < \mu^{\text{size}}_b(N)$.

\section{Evaluation of existing algorithms}

\subsection{Computational setup}
\label{sec:AlgComp}
To conduct experiments it was necessary to implement each of the algorithms and to run them on various environments. To achieve this I built a framework in Python, specifying base classes for algorithms, environments and each of their respective components. The structure of the framework is shown in figure \ref{fig:CodeStructure} Each of the algorithms were implemented according to the template laid out by the framework allowing for any algorithm to be easily paired with any environment. ES uses a neural network policy which was implemented in PyTorch \cite{PyTorch} to enable acceleration of computation. Other algorithms mainly used NumPy \cite{NumPy} and SciPy \cite{SciPy} for their calculations. To pair algorithms with environments I implemented a Universe class which given a number of algorithms, environments, and parameters, appropriately pairs and evaluates them while recording details of the training via its logger. This means it was possible to run experiments in a large array of configurations using just a few lines of code in a Jupyter notebook. Once the experiments were complete the results were saved in a standardized data structure, shown in figure \ref{fig:DataStructure} from which I could analyse the performance of the algorithms. I used a number of Gymnasium (formerly OpenAI Gym \cite{Gym}) environments to train on as it has many standard benchmarks for RL algorithms pre-implemented. To make them compatible with my framework I wrote a wrapper that mapped the inputs and outputs to an appropriate form.
As a separate module I built a data analysis tool to extract the desired data from the results of an experiment and plot the results. This often involved extracting and combining data from multiple different files.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{Figures/FrameworkStructure.png}
    \caption{The structure of the framework used to run the experiments}
    \label{fig:CodeStructure}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{Figures/DataStructure.png}
    \caption{The data structure used to store the results of experiments}
    \label{fig:DataStructure}
\end{figure}

\subsection{Experiments}
I evaluated the DQL, EBCDQL, DAVIA, and ES algorithms on the Simple Grid and Frozen Lake environments. They are both grid worlds where the goal is to move from a fixed starting state to a fixed terminal state and the possible actions are to move left, right, up, down, or stay in the same place. Simple Grid is a custom 5x5 grid world where the goal is to move from the top left state to the bottom right state, agents receive a reward of $-1$ for every step apart from at the terminal state where they receive a reward of $0$. Frozen lake (figure \ref{fig:FrozenLake}) is a 5x5 Grid environment from Gymnasium in which the goal is to move from the upper left to the lower right square upon which the agent receives a reward of $1$. A number of squares in the grid are holes in the lake, where if reached by the agent the episode ends. These environments were chosen as they were simple enough for all the algorithms to feasibly run on them while still providing a clear picture of each of their performance.

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{Figures/frozen_lake.png}
    \caption{The Frozen Lake environment}
    \label{fig:FrozenLake}
\end{figure}

Each experiment used $n=5$ agents and a discount factor of $\gamma=0.9$. Agents were trained over 1000 episodes and this was repeated across 100 trials. I analyse the average discounted episodic reward ($\mu^{\text{reward}}$), the average number of messages ($\mu^{\text{frequency}}$), and the average size of messages ($\mu^{\text{size}}$). I plot the mean as well as the $10^{\text{th}}$ to $90^{\text{th}}$ percentile across trials for each algorithm.

The results on the Simple Grid environment are shown in figure \ref{fig:EvalSG}. It can be seen that DQL and DAVIA quickly converge to an optimal solution whereupon DAVIA rapidly reduces communication. EBCDQL reaches a suboptimal solution at which communication reduces significantly. This means the TD errors observed beyond this point are not large enough to justify communication even when improved performance is possible, resulting in a stagnation of learning. ES fails to learn an effective strategy for reaching the terminal, achieving near the minimum reward on every episode. This is likely due to a lack of exploration and low sensitivity to rewards when they are achieved. In this case the discount factor of $\gamma=0.9$ means that rewards towards the end of the trajectory have little impact resulting in a very small difference in rewards between agents, so exploration is only down to the randomly chosen perturbations and the approximation of the policy gradient is poor.

For ES and DQL the number of messages sent is, by design, one per agent per episode which in this setting is the worst achievable. For EBCDQL and DAVIA the number of messages sent decreases rapidly around episode 50 meaning fewer agents are receiving significant updates to their value functions each episode.

As DQL sends the whole trajectory after every episode, for long episode lengths the size of messages it sends is poor. This can be seen at the beginning of figure \ref{fig:EvalSGSizeMessages}. However, as the length of the episodes decrease, as faster routes to the terminal have been found, the size of the messages decreases dramatically. A similar behaviour is exhibited by EBCDQL, however, only significant subsections of the trajectory are sent, so the sizes of the messages are smaller. For DAVIA, since only the value gradient is communicated, the size of the messages sent are constant so the change in the average size of messages sent is only due to variations in the number of agents communicating. ES only communicates a single scalar per agent per episode, so the average size of messages is very small, but constant.

The results on the Frozen Lake environment are shown in figure \ref{fig:EvalFL}. Figure \ref{fig:EvalFLReward} shows the cumulative episodic reward rather than the episodic reward as the results are particularly noisy meaning the cumulative plot more clearly shows the learning of the agents. By looking at the gradient we can determine how well the algorithms learned. EBCDQL performed best followed by ES, however, after around episode 200 their respective plots become linear meaning no learning occurs from this point on. DQL and DAVIA both failed to reach the terminal state a significant number of times.

For EBCDQL and DAVIA in this environment communication was minimal. This is perhaps because achieving any reward takes a large amount of exploration and thus episodes that could provide a significant update to the value function are rare. The relationship between the number of messages and the size of the messages remained similar to that demonstrated in Simple Grid.



\begin{figure}[H]
    \centering
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/AlgEval/SimpleGridC_2/episodic_reward.png}
        \caption{The average episodic reward received by agents}
        \label{fig:EvalSGReward}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/AlgEval/SimpleGridC_2/num_messages.png}
        \caption{The average number of messages sent from agents to central learner}
        \label{fig:EvalSGNumMessages}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/AlgEval/SimpleGridC_2/size_messages.png}
        \caption{The average size of messages sent from agents to central learner}
        \label{fig:EvalSGSizeMessages}
    \end{subfigure}
    \caption{Results from the algorithm evaluation in the Simple Grid environment for DAVIA, DQL, EBCDQL, and ES with $n=5$ over 1000 episodes and 100 trials. The shading represents the $10^\text{th}$ to the $90^{\text{th}}$ percentile across trials.}
    \label{fig:EvalSG}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/AlgEval/FrozenLakeC_2/reward.png}
        \caption{The cumulative average reward received by agents}
        \label{fig:EvalFLReward}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/AlgEval/FrozenLakeC_2/num_messages.png}
        \caption{The average number of messages sent from agents to central learner}
        \label{fig:EvalFLNumMessages}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/AlgEval/FrozenLakeC_2/size_messages.png}
        \caption{The average size of messages sent from agents to central learner}
        \label{fig:EvalFLSizeMessages}
    \end{subfigure}
    \caption{Results from the algorithm evaluation in the Frozen Lake environment for DAVIA, DQL, EBCDQL, and ES with $n=5$ over 1000 episodes and 100 trials. The shading represents the $10^\text{th}$ to the $90^{\text{th}}$ percentile across trials.}
    \label{fig:EvalFL}
\end{figure}

\section{Limitations}
These experiments were heavily limited by the time it took to run them, this resulted in it being difficult to effectively tune the hyperparameters for each algorithm. Finer tuning could lead to a more accurate comparison of what these algorithms are capable of. In particular EBCDQL and DAVIA could be tuned to communicate more on Frozen Lake which could improve performance.
In addition to this I only tested on two fairly similar environments. On other environments, performance could differ, therefore, using a larger set of environments with more variety between them could give a better picture of algorithm performance. Lastly the experiment consisted of 2 thresholded algorithms but only one black-box algorithm and no gradient compression algorithms. Using a wider variety of algorithms would provide a clearer picture of the relative performance of algorithms within each category and of the performance of each category relative to the others.

\section{Summary}
In summary, we have observed that 
\begin{itemize}
    \item EBCDQL and DAVIA are effective algorithms. They achieve mostly good rewards and their thresholded communication show a good trade off between performance and communication. They send messages only when it provides benefit to the learning process
    \item DQL learns well but has prohibitively poor communication properties. It sends a message every episode which is proportional to the length of the episode
    \item ES is not effective in these environments but has excellent performance in terms of the size of messages sent
\end{itemize}

ES performs poorly in these experiments proving to be brittle in simple environments. However, ES is considerably more flexible than any of the other algorithms tested here. Its effectiveness on environments infamous for their difficulty such as the MuJoCo humanoid shows the performance exhibited here does not well represent its strengths. The size of messages sent are very small, however, the frequency of the messages is poor. One way to improve this is to use a probabilistic communication scheme to reduce the number of messages sent. This is the focus of the subsequent chapter.

% As the simplest algorithm, not designed for communication efficiency, DQL provides a baseline for the other algorithms to compare against. We see that in simple Grid it performes well in terms of quality of learning but proves ineffective on the Frozen lake environment in this regard. Its communication performance is poor with whole trajectories being sent every episode. This is particularly inefficient as the size of these messages grows with episode length. It is also in violation of the prohibition of communication of raw data by FRL. The tabular value function means DQL is limited to use on only simple environments. To improve performance and flexibility one could make use of an approximate value function.

% EBCDQL performs marginally worse than DQL on the Simple Grid environment and better on the Frozen Lake environment. In both cases communication performance is greatly improved due to its adaptive communication scheme without negatively impacting learning performance. However, it is also limited by a tabular value function, scaling poorly with the size of the environment. An adaptation of this method to that with an approximated value function through deep Q learning could prove efficient and capable.

% DAVIA stands out in both learning and communication performance on Simple Grid. It effectively learns a strategy for reaching the terminal and ceases communication after it has done so. On Frozen Lake it did not learn an effective strategy, as it rarely communicated. This could perhaps be remedied by using different basis functions or fine-tuning the hyperparameters. It benefits from the use of an approximated value function meaning it is much more flexible to the size of environments than DQL and EBCDQL. As well as this it enables the use of constant sized messages in the form of value gradients. Extension of this method to non-linear approximations of the value function would create the opportunity apply the communication efficiency of this algorithm to more complex problems.

